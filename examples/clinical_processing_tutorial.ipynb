{"cells":[{"cell_type":"markdown","metadata":{"id":"SSwUp_4Lws9u"},"source":["# HoneyBee Clinical Processing Tutorial\n","\n","This notebook provides a comprehensive exploration of HoneyBee's clinical data processing pipeline, demonstrating all key features including:\n","\n","- Document processing (PDF, images, EHR formats)\n","- OCR capabilities with medical terminology verification\n","- Integration with biomedical language models\n","- Clinical entity recognition and normalization\n","- Temporal timeline extraction\n","- Cancer-specific entity extraction\n","- Ontology linking\n","\n","## Table of Contents\n","\n","1. [Setup and Imports](#setup)\n","2. [Basic Usage](#basic-usage)\n","3. [Custom Configuration](#custom-config)\n","4. [Processing Raw Text](#raw-text)\n","5. [Batch Processing](#batch-processing)\n","6. [Advanced Tokenization](#advanced-tokenization)\n","7. [Cancer-Specific Entity Extraction](#cancer-entities)\n","8. [Temporal Timeline Construction](#temporal-timeline)\n","9. [Entity Normalization and Ontology Linking](#ontology-linking)\n","10. [PDF and OCR Processing](#pdf-processing)\n","11. [Visualization and Analysis](#visualization)"]},{"cell_type":"markdown","metadata":{"id":"qnFTGATsws9v"},"source":["## 1. Setup and Imports <a id='setup'></a>"]},{"cell_type":"markdown","source":["### a.  Setup Google Drive and Clone HoneyBee Git Repo"],"metadata":{"id":"dzKfe7VnzQvO"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"T3wWAFziyjbH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","#### **NOTE:** Ignore '/content/drive/MyDrive/HoneyBee_MayoWorkshop/HoneyBee/.git/hooks/post-checkout': Permission denied. This can happen in environments like Colab when interacting with mounted file systems like Google Drive due to how permissions are handled. The core git clone command likely completed successfully, but this specific script failed to execute. For many repositories, the default hooks are not critical for basic usage. You can confirm if the cloning was successful by listing the contents of the HoneyBee_MayoWorkshop directory in your Google Drive. If the HoneyBee directory exists there, the cloning itself was mostly successful.\n","\n","\n","---\n","\n"],"metadata":{"id":"SEMR3e2R1AFF"}},{"cell_type":"code","source":["%%bash\n","cd /content/drive/MyDrive/\n","mkdir -p HoneyBee_MayoWorkshop\n","cd HoneyBee_MayoWorkshop\n","git clone https://github.com/lab-rasool/HoneyBee-Workshop.git"],"metadata":{"id":"pFaLP5CHy_Rr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### b. Install Dependencies"],"metadata":{"id":"pRRd72t00lyK"}},{"cell_type":"code","source":["!apt-get update\n","!apt-get install -y openslide-tools tesseract-ocr"],"metadata":{"id":"4TNtju_40fTQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standard library imports\n","import json\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from IPython.display import display, HTML\n","import sys\n","sys.path.append('/content/drive/MyDrive/HoneyBee_MayoWorkshop/HoneyBee-Workshop')"],"metadata":{"id":"BCsUx30f1wfw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### c. Install packages, Restart the Session if asked and rerun all previous cells again"],"metadata":{"id":"vqUnE-g73Uox"}},{"cell_type":"code","source":["!pip install -r /content/drive/MyDrive/HoneyBee_MayoWorkshop/HoneyBee-Workshop/requirements.txt"],"metadata":{"id":"C_05C63p1rf-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# if any of the below mentioned package installation fails, uncomment the following line to install that package\n","#!pip install SimpleITK pytesseract PyPDF2 pdf2image pydicom"],"metadata":{"id":"TcOyh-zo3LOq"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8gcefYows9v"},"outputs":[],"source":["from honeybee.processors.clinical_processor import ClinicalProcessor\n","\n","# Set up visualization style\n","plt.style.use('seaborn-v0_8-darkgrid')\n","sns.set_palette(\"husl\")\n","\n","print(\"HoneyBee Clinical Processing Pipeline loaded successfully!\")"]},{"cell_type":"markdown","metadata":{"id":"6Fx9aQaews9w"},"source":["## 2. Basic Usage <a id='basic-usage'></a>\n","\n","Let's start with the simplest use case - processing a clinical document with default settings."]},{"cell_type":"markdown","source":["‚ùóGrant access if you get the following pop-up: ‚ùó\n","##  Notebook titled \"clinical_processing_tutorial.ipynb\" does not have access to secret named \"HF_TOKEN\". Grant access?"],"metadata":{"id":"FEhku_kmTnOa"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lzk6tPkVws9w"},"outputs":[],"source":["# Initialize the clinical processor with default configuration\n","processor = ClinicalProcessor()\n","\n","# Sample clinical text\n","sample_clinical_text = \"\"\"\n","PATIENT: Jane Doe\n","DATE: 03/15/2023\n","\n","CHIEF COMPLAINT:\n","Follow-up for breast cancer treatment.\n","\n","HISTORY OF PRESENT ILLNESS:\n","Ms. Doe is a 55-year-old female diagnosed with invasive ductal carcinoma of the left breast\n","on 01/10/2023. Tumor measured 2.3 cm, Grade 2, ER positive (95%), PR positive (80%),\n","HER2 negative. Staging: T2N0M0, Stage IIA.\n","\n","TREATMENT HISTORY:\n","- Lumpectomy performed on 02/15/2023\n","- Started adjuvant tamoxifen 20mg daily on 03/01/2023\n","- Radiation therapy planned for next month\n","\n","CURRENT MEDICATIONS:\n","1. Tamoxifen 20mg PO daily\n","2. Metformin 500mg PO BID\n","3. Lisinopril 10mg PO daily\n","\n","LABORATORY RESULTS:\n","WBC: 6.8 K/¬µL (normal 4.5-11.0)\n","Hemoglobin: 13.2 g/dL (normal 12.0-16.0)\n","Platelets: 225 K/¬µL (normal 150-400)\n","Creatinine: 0.8 mg/dL (normal 0.6-1.2)\n","\"\"\"\n","\n","# Process the clinical text\n","result = processor.process_text(sample_clinical_text, document_type=\"clinical_note\")\n","\n","# Display basic results\n","print(\"üìã Processing Results:\")\n","print(f\"- Text length: {len(result.get('text', ''))} characters\")\n","print(f\"- Entities extracted: {len(result.get('entities', []))}\")\n","print(f\"- Timeline events: {len(result.get('temporal_timeline', []))}\")\n","print(f\"- Document sections: {len(result.get('document_structure', {}).get('sections', {}))}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC00qPh9ws9x"},"outputs":[],"source":["# Display extracted entities\n","print(\"\\nüîç Extracted Entities:\")\n","print(\"=\" * 60)\n","\n","entities_df = pd.DataFrame(result.get('entities', []))\n","if not entities_df.empty:\n","    # Group entities by type\n","    entity_types = entities_df.groupby('type').size().sort_values(ascending=False)\n","\n","    for entity_type, count in entity_types.items():\n","        print(f\"\\n{entity_type.upper()} ({count}):\")\n","        type_entities = entities_df[entities_df['type'] == entity_type]\n","        for _, entity in type_entities.iterrows():\n","            print(f\"  ‚Ä¢ {entity['text']}\")\n","            if pd.notna(entity.get('properties')):\n","                props = entity['properties']\n","                if isinstance(props, dict):\n","                    for key, value in props.items():\n","                        if key not in ['source', 'pattern']:\n","                            print(f\"    - {key}: {value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifc07CGPws9y"},"outputs":[],"source":["# Display temporal timeline\n","print(\"\\nüìÖ Temporal Timeline:\")\n","print(\"=\" * 60)\n","\n","timeline = result.get('temporal_timeline', [])\n","if timeline:\n","    timeline_df = pd.DataFrame(timeline)\n","\n","    for idx, event in enumerate(timeline):\n","        print(f\"\\nEvent {idx + 1}:\")\n","        print(f\"  Date text: {event['temporal_text']}\")\n","        if event.get('normalized_date'):\n","            print(f\"  Normalized: {event['normalized_date']}\")\n","\n","        # Show related clinical information\n","        if event.get('related_entities'):\n","            print(\"  Related clinical information:\")\n","            for entity_idx in event['related_entities']:\n","                if entity_idx < len(result['entities']):\n","                    entity = result['entities'][entity_idx]\n","                    print(f\"    - {entity['type']}: {entity['text']}\")\n","else:\n","    print(\"No temporal events found in the document.\")"]},{"cell_type":"markdown","metadata":{"id":"gR3RZviWws9y"},"source":["## 3. Custom Configuration <a id='custom-config'></a>\n","\n","Now let's explore custom configurations for different use cases."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9CzL_nSiws9y"},"outputs":[],"source":["# Configuration for detailed cancer analysis\n","cancer_config = {\n","    \"document_processor\": {\n","        \"use_ocr\": True,\n","        \"use_ehr\": True\n","    },\n","    \"tokenization\": {\n","        \"model\": \"gatortron\",  # Best for clinical notes\n","        \"max_length\": 512,\n","        \"segment_strategy\": \"sentence\",\n","        \"long_document_strategy\": \"sliding_window\"\n","    },\n","    \"entity_recognition\": {\n","        \"use_rules\": True,\n","        \"use_patterns\": True,\n","        \"cancer_specific_extraction\": True,\n","        \"temporal_extraction\": True,\n","        \"abbreviation_expansion\": True,\n","        \"ontologies\": [\"snomed_ct\", \"rxnorm\", \"loinc\"]\n","    },\n","    \"processing_pipeline\": [\"document\", \"tokenization\", \"entity_recognition\"],\n","    \"output\": {\n","        \"include_raw_text\": True,\n","        \"include_tokens\": True,\n","        \"include_entities\": True,\n","        \"include_document_structure\": True,\n","        \"include_temporal_timeline\": True\n","    }\n","}\n","\n","# Initialize processor with custom configuration\n","cancer_processor = ClinicalProcessor(config=cancer_config)\n","\n","print(\"‚úÖ Custom processor initialized with cancer-specific configuration\")\n","print(\"\\nConfiguration highlights:\")\n","print(f\"- Model: {cancer_config['tokenization']['model']}\")\n","print(f\"- Cancer-specific extraction: {cancer_config['entity_recognition']['cancer_specific_extraction']}\")\n","print(f\"- Ontologies: {', '.join(cancer_config['entity_recognition']['ontologies'])}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpvX1kJUws9z"},"outputs":[],"source":["# Compare different model configurations\n","models_to_compare = [\n","    {\"name\": \"Bio-ClinicalBERT\", \"model\": \"bioclinicalbert\"},\n","    {\"name\": \"PubMedBERT\", \"model\": \"pubmedbert\"},\n","    {\"name\": \"GatorTron\", \"model\": \"gatortron\"},\n","]\n","\n","comparison_results = []\n","\n","for model_info in models_to_compare:\n","    config = {\n","        \"tokenization\": {\n","            \"model\": model_info[\"model\"],\n","            \"max_length\": 512\n","        },\n","        \"entity_recognition\": {\n","            \"cancer_specific_extraction\": True\n","        }\n","    }\n","\n","    try:\n","        processor = ClinicalProcessor(config=config)\n","        result = processor.process_text(sample_clinical_text, \"clinical_note\")\n","\n","        comparison_results.append({\n","            \"Model\": model_info[\"name\"],\n","            \"Entities\": len(result.get('entities', [])),\n","            \"Timeline Events\": len(result.get('temporal_timeline', [])),\n","            \"Status\": \"‚úÖ Success\"\n","        })\n","    except Exception as e:\n","        comparison_results.append({\n","            \"Model\": model_info[\"name\"],\n","            \"Entities\": 0,\n","            \"Timeline Events\": 0,\n","            \"Status\": f\"‚ùå Error: {str(e)[:30]}...\"\n","        })\n","\n","# Display comparison\n","comparison_df = pd.DataFrame(comparison_results)\n","display(HTML(comparison_df.to_html(index=False)))"]},{"cell_type":"markdown","metadata":{"id":"j2WzCfIPws9z"},"source":["## 4. Processing Raw Text <a id='raw-text'></a>\n","\n","Process different types of clinical documents directly from text."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSBo5DNTws90"},"outputs":[],"source":["# Different types of clinical documents\n","clinical_documents = {\n","    \"progress_note\": \"\"\"\n","Patient presents with stage III non-small cell lung cancer.\n","EGFR mutation positive (exon 19 deletion).\n","Started on erlotinib 150mg daily.\n","Partial response observed after 3 months of treatment.\n","Mild rash managed with topical corticosteroids.\n","\"\"\",\n","\n","    \"pathology_report\": \"\"\"\n","DIAGNOSIS:\n","Invasive ductal carcinoma, Grade 2, measuring 2.1 cm.\n","ER positive (90%), PR positive (75%), HER2 negative.\n","T2N0M0 stage IIA. Margins clear.\n","Ki-67 proliferation index: 15%\n","\"\"\",\n","\n","    \"radiology_report\": \"\"\"\n","CT CHEST WITH CONTRAST\n","INDICATION: Follow-up lung cancer\n","FINDINGS:\n","Right upper lobe mass measures 3.2 x 2.8 cm, decreased from 4.5 x 3.9 cm.\n","No new pulmonary nodules. No pleural effusion.\n","IMPRESSION: Partial response to treatment.\n","\"\"\"\n","}\n","\n","# Process each document type\n","processor = ClinicalProcessor()\n","document_results = {}\n","\n","for doc_type, text in clinical_documents.items():\n","    result = processor.process_text(text, document_type=doc_type)\n","    document_results[doc_type] = result\n","\n","    print(f\"\\nüìÑ {doc_type.replace('_', ' ').title()}:\")\n","    print(\"=\" * 50)\n","\n","    # Extract key information based on document type\n","    entities = result.get('entities', [])\n","\n","    if doc_type == \"pathology_report\":\n","        # Focus on tumor characteristics\n","        tumor_info = [e for e in entities if e['type'] in ['tumor', 'staging', 'biomarker']]\n","        print(\"Tumor Characteristics:\")\n","        for entity in tumor_info:\n","            print(f\"  ‚Ä¢ {entity['type']}: {entity['text']}\")\n","\n","    elif doc_type == \"progress_note\":\n","        # Focus on treatment and response\n","        treatment_info = [e for e in entities if e['type'] in ['medication', 'response', 'condition']]\n","        print(\"Treatment Information:\")\n","        for entity in treatment_info:\n","            print(f\"  ‚Ä¢ {entity['type']}: {entity['text']}\")\n","\n","    elif doc_type == \"radiology_report\":\n","        # Focus on measurements and findings\n","        findings = [e for e in entities if e['type'] in ['measurement', 'condition', 'response']]\n","        print(\"Key Findings:\")\n","        for entity in findings:\n","            print(f\"  ‚Ä¢ {entity['type']}: {entity['text']}\")"]},{"cell_type":"markdown","metadata":{"id":"TTxXTfUBws90"},"source":["## 5. Batch Processing <a id='batch-processing'></a>\n","\n","Process multiple clinical documents efficiently in batch mode."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcukVM7tws90"},"outputs":[],"source":["# Create a temporary directory with sample files for batch processing\n","import tempfile\n","import os\n","\n","# Create temporary directory\n","temp_dir = tempfile.mkdtemp(prefix=\"clinical_batch_\")\n","print(f\"Created temporary directory: {temp_dir}\")\n","\n","# Create sample clinical documents\n","sample_docs = [\n","    (\"patient_001_note.txt\", \"Patient 001: Breast cancer, Stage II, ER/PR positive, started tamoxifen.\"),\n","    (\"patient_002_note.txt\", \"Patient 002: Lung cancer, EGFR positive, on erlotinib 150mg daily.\"),\n","    (\"patient_003_note.txt\", \"Patient 003: Colorectal cancer, KRAS wild-type, receiving FOLFOX chemotherapy.\"),\n","    (\"patient_004_note.txt\", \"Patient 004: Melanoma, BRAF V600E mutation, started dabrafenib and trametinib.\"),\n","]\n","\n","# Write sample files\n","for filename, content in sample_docs:\n","    filepath = os.path.join(temp_dir, filename)\n","    with open(filepath, 'w') as f:\n","        f.write(content)\n","    print(f\"Created: {filename}\")\n","\n","# Process batch\n","processor = ClinicalProcessor()\n","batch_results = processor.process_batch(\n","    input_dir=temp_dir,\n","    file_pattern=\"*.txt\",\n","    save_output=False  # Don't save JSON files for this demo\n",")\n","\n","print(f\"\\n‚úÖ Processed {len(batch_results)} documents\")\n","\n","# Analyze batch results\n","batch_summary = []\n","for result in batch_results:\n","    stats = processor.get_summary_statistics(result)\n","    batch_summary.append({\n","        \"File\": Path(result['file_path']).name,\n","        \"Characters\": stats['text_length'],\n","        \"Entities\": stats['num_entities'],\n","        \"Entity Types\": len(stats['entity_types']),\n","        \"Top Entity Type\": max(stats['entity_types'].items(), key=lambda x: x[1])[0] if stats['entity_types'] else \"None\"\n","    })\n","\n","# Display summary\n","batch_df = pd.DataFrame(batch_summary)\n","display(HTML(batch_df.to_html(index=False)))\n","\n","# Clean up\n","import shutil\n","shutil.rmtree(temp_dir)\n","print(f\"\\nCleaned up temporary directory\")"]},{"cell_type":"markdown","metadata":{"id":"vNss7hnyws90"},"source":["## 6. Advanced Tokenization <a id='advanced-tokenization'></a>\n","\n","Handle long documents with various tokenization strategies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dOmp1iXws90"},"outputs":[],"source":["# Create a long clinical document\n","long_clinical_text = \"\"\"\n","COMPREHENSIVE ONCOLOGY REPORT\n","\n","PATIENT IDENTIFICATION:\n","Name: John Smith\n","MRN: 12345678\n","Date of Birth: 01/15/1960\n","Date of Report: 11/15/2023\n","\n","DIAGNOSIS AND STAGING:\n","Primary Diagnosis: Adenocarcinoma of the lung, right upper lobe\n","Date of Diagnosis: 08/20/2023\n","Histologic Grade: Moderately differentiated (Grade 2)\n","Clinical Stage at Diagnosis: cT2aN1M0, Stage IIB\n","Pathologic Stage: pT2aN1M0, Stage IIB\n","\n","MOLECULAR PROFILE:\n","EGFR: Wild type\n","ALK: Negative by FISH\n","ROS1: Negative\n","BRAF: Wild type\n","PD-L1: 45% tumor proportion score\n","TMB: 12 mutations/Mb (intermediate)\n","\n","TREATMENT HISTORY:\n","1. Surgical Resection (09/15/2023):\n","   - Right upper lobectomy with mediastinal lymph node dissection\n","   - Pathology: 3.2 cm adenocarcinoma, margins negative\n","   - Lymph nodes: 2/15 positive (stations 10R and 11R)\n","\n","2. Adjuvant Chemotherapy (10/15/2023 - 01/15/2024):\n","   - Regimen: Carboplatin AUC 5 + Pemetrexed 500 mg/m2\n","   - Cycles completed: 4 of 4 planned\n","   - Tolerance: Grade 2 fatigue, Grade 1 nausea\n","\n","3. Maintenance Therapy (02/01/2024 - ongoing):\n","   - Pembrolizumab 200 mg IV every 3 weeks\n","   - Cycles completed: 8 to date\n","   - Tolerance: Grade 1 arthralgia, no immune-related adverse events\n","\n","RESPONSE ASSESSMENT:\n","Most recent imaging (10/30/2023):\n","- CT chest/abdomen/pelvis: No evidence of disease\n","- Brain MRI: Negative for metastases\n","- PET/CT: No FDG-avid disease\n","\n","CURRENT STATUS:\n","- Disease status: No evidence of disease (NED)\n","- Performance status: ECOG 0\n","- Next imaging: 01/30/2024\n","\n","RECOMMENDATIONS:\n","1. Continue pembrolizumab maintenance therapy\n","2. Surveillance imaging every 3 months\n","3. Monitor for immune-related adverse events\n","4. Annual brain MRI given stage at diagnosis\n","\"\"\" * 3  # Make it even longer by repeating\n","\n","# Test different tokenization strategies\n","tokenization_strategies = [\n","    {\"name\": \"Sliding Window\", \"strategy\": \"sliding_window\"},\n","    {\"name\": \"Hierarchical\", \"strategy\": \"hierarchical\"},\n","    {\"name\": \"Important Segments\", \"strategy\": \"important_segments\"},\n","]\n","\n","tokenization_results = []\n","\n","for strategy_info in tokenization_strategies:\n","    config = {\n","        \"tokenization\": {\n","            \"model\": \"gatortron\",\n","            \"max_length\": 512,\n","            \"segment_strategy\": \"paragraph\",\n","            \"long_document_strategy\": strategy_info[\"strategy\"],\n","            \"stride\": 128\n","        }\n","    }\n","\n","    processor = ClinicalProcessor(config=config)\n","    result = processor.process_text(long_clinical_text, \"comprehensive_report\")\n","\n","    tokenization = result.get('tokenization', {})\n","\n","    tokenization_results.append({\n","        \"Strategy\": strategy_info[\"name\"],\n","        \"Windows/Sections\": tokenization.get('num_windows', tokenization.get('num_segments_selected', len(tokenization.get('sections', {})))),\n","        \"Total Tokens\": tokenization.get('num_tokens', 'N/A'),\n","        \"Strategy Used\": tokenization.get('tokenization_strategy', 'unknown')\n","    })\n","\n","    print(f\"\\n{strategy_info['name']} Strategy:\")\n","    print(\"=\" * 40)\n","\n","    if strategy_info[\"strategy\"] == \"sliding_window\" and 'num_windows' in tokenization:\n","        print(f\"Created {tokenization['num_windows']} overlapping windows\")\n","        print(f\"Stride: {config['tokenization']['stride']} tokens\")\n","\n","    elif strategy_info[\"strategy\"] == \"hierarchical\" and 'sections' in tokenization:\n","        print(f\"Document sections: {len(tokenization['sections'])}\")\n","        for section_name in list(tokenization['sections'].keys())[:3]:\n","            print(f\"  - {section_name}\")\n","\n","    elif strategy_info[\"strategy\"] == \"important_segments\":\n","        selected = tokenization.get('num_segments_selected', 0)\n","        total = tokenization.get('num_segments_total', 0)\n","        print(f\"Selected {selected} of {total} segments based on clinical relevance\")\n","\n","# Display comparison\n","tokenization_df = pd.DataFrame(tokenization_results)\n","display(HTML(tokenization_df.to_html(index=False)))"]},{"cell_type":"markdown","metadata":{"id":"L293EhCJws91"},"source":["## 7. Cancer-Specific Entity Extraction <a id='cancer-entities'></a>\n","\n","Extract detailed oncology-specific information."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moropNwOws91"},"outputs":[],"source":["# Configure for cancer-specific extraction\n","cancer_config = {\n","    \"entity_recognition\": {\n","        \"cancer_specific_extraction\": True,\n","        \"temporal_extraction\": True,\n","        \"ontologies\": [\"snomed_ct\", \"rxnorm\"]\n","    }\n","}\n","\n","processor = ClinicalProcessor(config=cancer_config)\n","\n","# Comprehensive pathology report\n","pathology_text = \"\"\"\n","SURGICAL PATHOLOGY REPORT\n","\n","FINAL DIAGNOSIS:\n","1. RIGHT BREAST, UPPER OUTER QUADRANT, EXCISION:\n","   - INVASIVE DUCTAL CARCINOMA, NOTTINGHAM GRADE 2 (3+2+1=6/9)\n","   - TUMOR SIZE: 2.1 cm in greatest dimension\n","   - MARGINS: Negative for invasive carcinoma (closest margin 3 mm)\n","   - LYMPHOVASCULAR INVASION: Present, focal\n","\n","2. SENTINEL LYMPH NODES (3), RIGHT AXILLA:\n","   - ONE OF THREE LYMPH NODES POSITIVE FOR METASTATIC CARCINOMA\n","   - Largest metastatic deposit: 4 mm\n","   - Extranodal extension: Not identified\n","\n","IMMUNOHISTOCHEMISTRY:\n","- Estrogen Receptor (ER): Positive (90% of tumor cells, strong intensity)\n","- Progesterone Receptor (PR): Positive (75% of tumor cells, moderate intensity)\n","- HER2/neu: Negative (1+, not amplified by FISH)\n","- Ki-67 Proliferation Index: 15%\n","\n","ONCOTYPE DX RECURRENCE SCORE: 18 (Intermediate risk)\n","\n","PATHOLOGIC STAGING (AJCC 8th Edition):\n","- Primary Tumor (pT): pT2 (tumor 2.1 cm)\n","- Regional Lymph Nodes (pN): pN1a (1-3 axillary lymph nodes)\n","- Distant Metastasis (pM): pM0\n","- Stage Group: IIB (pT2 pN1a M0)\n","\"\"\"\n","\n","result = processor.process_text(pathology_text, \"pathology_report\")\n","\n","# Organize entities by cancer-specific categories\n","cancer_entities = {\n","    \"Tumor Characteristics\": [],\n","    \"Biomarkers\": [],\n","    \"Staging\": [],\n","    \"Measurements\": [],\n","    \"Response/Risk\": []\n","}\n","\n","for entity in result.get('entities', []):\n","    if entity['type'] == 'tumor':\n","        cancer_entities[\"Tumor Characteristics\"].append(entity)\n","    elif entity['type'] == 'biomarker':\n","        cancer_entities[\"Biomarkers\"].append(entity)\n","    elif entity['type'] == 'staging':\n","        cancer_entities[\"Staging\"].append(entity)\n","    elif entity['type'] == 'measurement' and 'size' in entity.get('properties', {}):\n","        cancer_entities[\"Measurements\"].append(entity)\n","    elif entity['type'] == 'response':\n","        cancer_entities[\"Response/Risk\"].append(entity)\n","\n","# Display organized cancer entities\n","print(\"üî¨ Cancer-Specific Entity Extraction Results:\")\n","print(\"=\" * 60)\n","\n","for category, entities in cancer_entities.items():\n","    if entities:\n","        print(f\"\\n{category}:\")\n","        for entity in entities:\n","            print(f\"  ‚Ä¢ {entity['text']}\")\n","            props = entity.get('properties', {})\n","            if isinstance(props, dict):\n","                # Display relevant properties\n","                if 'biomarker' in props:\n","                    print(f\"    - Biomarker: {props['biomarker']}\")\n","                if 'status' in props:\n","                    print(f\"    - Status: {props['status']}\")\n","                if 'size' in props:\n","                    print(f\"    - Size: {props['size']} {props.get('unit', '')}\")\n","                if 'stage' in props:\n","                    print(f\"    - Stage: {props['stage']}\")\n","                if 'ontology_links' in props:\n","                    for link in props['ontology_links']:\n","                        print(f\"    - {link['ontology']}: {link['concept_name']}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A0Vrp_P8ws91"},"outputs":[],"source":["# Visualize biomarker distribution\n","biomarkers = cancer_entities[\"Biomarkers\"]\n","if biomarkers:\n","    biomarker_data = []\n","    for b in biomarkers:\n","        props = b.get('properties', {})\n","        if isinstance(props, dict) and 'biomarker' in props:\n","            biomarker_data.append({\n","                'Biomarker': props.get('biomarker', 'Unknown'),\n","                'Status': props.get('status', 'Unknown'),\n","                'Text': b['text']\n","            })\n","\n","    if biomarker_data:\n","        biomarker_df = pd.DataFrame(biomarker_data)\n","\n","        # Create visualization\n","        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n","\n","        # Biomarker counts\n","        biomarker_counts = biomarker_df['Biomarker'].value_counts()\n","        ax1.bar(biomarker_counts.index, biomarker_counts.values)\n","        ax1.set_title('Biomarker Mentions')\n","        ax1.set_xlabel('Biomarker')\n","        ax1.set_ylabel('Count')\n","        ax1.tick_params(axis='x', rotation=45)\n","\n","        # Status distribution\n","        status_counts = biomarker_df['Status'].value_counts()\n","        colors = ['green' if 'positive' in s.lower() else 'red' if 'negative' in s.lower() else 'gray'\n","                 for s in status_counts.index]\n","        ax2.pie(status_counts.values, labels=status_counts.index, colors=colors, autopct='%1.1f%%')\n","        ax2.set_title('Biomarker Status Distribution')\n","\n","        plt.tight_layout()\n","        plt.show()\n","else:\n","    print(\"No biomarkers found for visualization\")"]},{"cell_type":"markdown","metadata":{"id":"YESCGgIKws91"},"source":["## 8. Temporal Timeline Construction <a id='temporal-timeline'></a>\n","\n","Extract and visualize patient treatment timelines."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTCMZQbRws91"},"outputs":[],"source":["# Clinical note with rich temporal information\n","timeline_text = \"\"\"\n","ONCOLOGY TREATMENT SUMMARY\n","\n","Patient: Sarah Johnson\n","DOB: 05/12/1965\n","\n","TIMELINE OF EVENTS:\n","\n","January 15, 2023: Initial presentation with palpable breast mass\n","January 20, 2023: Mammography and ultrasound performed\n","January 25, 2023: Core needle biopsy confirmed invasive ductal carcinoma\n","February 1, 2023: Genetic testing revealed BRCA2 mutation\n","February 10, 2023: PET/CT staging showed no distant metastases\n","February 15, 2023: Multidisciplinary tumor board recommendation for neoadjuvant chemotherapy\n","\n","February 20, 2023: Started neoadjuvant chemotherapy (AC-T regimen)\n","- Doxorubicin 60 mg/m2 + Cyclophosphamide 600 mg/m2 every 2 weeks x 4 cycles\n","- Completed on April 15, 2023\n","\n","April 20, 2023: Started weekly paclitaxel 80 mg/m2\n","- Completed 12 weeks on July 10, 2023\n","\n","July 25, 2023: Restaging showed partial response (tumor 4.2cm ‚Üí 1.8cm)\n","\n","August 15, 2023: Bilateral mastectomy with immediate reconstruction\n","August 22, 2023: Pathology showed residual invasive carcinoma 1.5 cm, margins clear\n","\n","September 15, 2023: Started adjuvant capecitabine due to residual disease\n","Currently on cycle 4 of 8 planned cycles\n","\n","October 1, 2023: Initiated endocrine therapy with tamoxifen\n","Planned duration: 10 years given BRCA2 mutation\n","\n","NEXT STEPS:\n","- Complete capecitabine in January 2024\n","- Consider prophylactic bilateral salpingo-oophorectomy given BRCA2\n","- Surveillance with clinical exams every 3 months\n","\"\"\"\n","\n","processor = ClinicalProcessor()\n","result = processor.process_text(timeline_text, \"treatment_summary\")\n","\n","# Extract and display timeline\n","timeline = result.get('temporal_timeline', [])\n","\n","print(\"üìÖ Patient Treatment Timeline:\")\n","print(\"=\" * 60)\n","\n","# Sort timeline by date if normalized dates are available\n","timeline_sorted = sorted(timeline, key=lambda x: x.get('normalized_date', '9999'))\n","\n","# Create timeline visualization data\n","timeline_data = []\n","\n","for event in timeline_sorted:\n","    print(f\"\\nüìå {event['temporal_text']}\")\n","    if event.get('normalized_date'):\n","        print(f\"   Date: {event['normalized_date']}\")\n","\n","    # Get related clinical events\n","    related_info = []\n","    for idx in event.get('related_entities', []):\n","        if idx < len(result['entities']):\n","            entity = result['entities'][idx]\n","            related_info.append(f\"{entity['type']}: {entity['text']}\")\n","\n","    if related_info:\n","        print(\"   Related events:\")\n","        for info in related_info:\n","            print(f\"   - {info}\")\n","\n","    # Collect for visualization\n","    if event.get('normalized_date'):\n","        timeline_data.append({\n","            'date': event['normalized_date'],\n","            'event': event['temporal_text'],\n","            'details': ', '.join(related_info[:2])  # Limit to 2 items for readability\n","        })"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4xybq3-ws92"},"outputs":[],"source":["# Create timeline visualization\n","if timeline_data:\n","    # Convert to DataFrame and parse dates\n","    timeline_df = pd.DataFrame(timeline_data)\n","    timeline_df['date'] = pd.to_datetime(timeline_df['date'], errors='coerce')\n","    timeline_df = timeline_df.dropna(subset=['date']).sort_values('date')\n","\n","    if not timeline_df.empty:\n","        # Create figure\n","        fig, ax = plt.subplots(figsize=(12, 8))\n","\n","        # Plot timeline\n","        dates = timeline_df['date']\n","        levels = np.arange(len(dates))\n","\n","        # Create the timeline\n","        ax.scatter(dates, levels, s=120, c='red', zorder=2)\n","\n","        # Add vertical line\n","        ax.vlines(dates, 0, levels, colors='gray', linestyles='dashed', alpha=0.5)\n","\n","        # Add event labels\n","        for i, (date, event, details) in enumerate(zip(dates, timeline_df['event'], timeline_df['details'])):\n","            # Alternate label positions for readability\n","            ha = 'right' if i % 2 == 0 else 'left'\n","            x_offset = -10 if i % 2 == 0 else 10\n","\n","            ax.annotate(event[:50] + '...' if len(event) > 50 else event,\n","                       xy=(date, i),\n","                       xytext=(x_offset, 0),\n","                       textcoords='offset points',\n","                       ha=ha,\n","                       va='center',\n","                       bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n","                       fontsize=9)\n","\n","            # Add details below if available\n","            if details:\n","                ax.annotate(details[:40] + '...' if len(details) > 40 else details,\n","                           xy=(date, i),\n","                           xytext=(x_offset, -20),\n","                           textcoords='offset points',\n","                           ha=ha,\n","                           va='top',\n","                           fontsize=7,\n","                           style='italic',\n","                           color='gray')\n","\n","        # Formatting\n","        ax.set_ylim(-1, len(dates))\n","        ax.set_xlabel('Date', fontsize=12)\n","        ax.set_title('Patient Treatment Timeline', fontsize=16, fontweight='bold')\n","        ax.grid(True, axis='x', alpha=0.3)\n","        ax.set_yticks([])\n","\n","        # Format x-axis\n","        fig.autofmt_xdate()\n","\n","        plt.tight_layout()\n","        plt.show()\n","else:\n","    print(\"No timeline data available for visualization\")"]},{"cell_type":"markdown","metadata":{"id":"nN4ehywDws92"},"source":["## 9. Entity Normalization and Ontology Linking <a id='ontology-linking'></a>\n","\n","Normalize medical terms and link to standard ontologies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_SoNJdpmws92"},"outputs":[],"source":["# Configure with ontology support\n","ontology_config = {\n","    \"entity_recognition\": {\n","        \"ontologies\": [\"snomed_ct\", \"rxnorm\", \"loinc\"],\n","        \"abbreviation_expansion\": True,\n","        \"term_disambiguation\": True\n","    }\n","}\n","\n","processor = ClinicalProcessor(config=ontology_config)\n","\n","# Text with medical abbreviations and terms\n","medical_text = \"\"\"\n","CLINICAL SUMMARY:\n","Pt is a 62 yo male with hx of NSCLC, s/p lobectomy.\n","Currently on tamoxifen 20mg PO daily for breast ca prevention (strong FH).\n","Recent labs: WBC 8.2, Hgb 13.5, Plt 250, Cr 1.1\n","CT showed stable post-op changes, no mets.\n","Plan: Continue surveillance, f/u in 3 mos.\n","\"\"\"\n","\n","result = processor.process_text(medical_text)\n","\n","# Display entities with normalization\n","print(\"üîó Entity Normalization and Ontology Linking:\")\n","print(\"=\" * 60)\n","\n","normalized_entities = []\n","for entity in result.get('entities', []):\n","    props = entity.get('properties', {})\n","\n","    # Check for expanded abbreviations or ontology links\n","    if isinstance(props, dict) and ('expanded' in props or 'ontology_links' in props):\n","        normalized_entities.append(entity)\n","\n","if normalized_entities:\n","    for entity in normalized_entities:\n","        print(f\"\\nüìã Original: '{entity['text']}'\")\n","        print(f\"   Type: {entity['type']}\")\n","\n","        props = entity['properties']\n","\n","        # Show abbreviation expansion\n","        if 'expanded' in props:\n","            print(f\"   ‚úÖ Expanded to: '{props['expanded']}'\")\n","\n","        # Show ontology links\n","        if 'ontology_links' in props:\n","            print(\"   üîó Ontology mappings:\")\n","            for link in props['ontology_links']:\n","                print(f\"      - {link['ontology']}: {link['concept_name']} (ID: {link['concept_id']})\")\n","else:\n","    print(\"No normalized entities found in this example.\")\n","\n","# Show all entities for comparison\n","print(\"\\n\\nAll extracted entities:\")\n","entity_summary = []\n","for entity in result.get('entities', []):\n","    entity_summary.append({\n","        'Text': entity['text'],\n","        'Type': entity['type'],\n","        'Normalized': '‚úÖ' if entity.get('properties', {}).get('ontology_links') else '‚ùå'\n","    })\n","\n","if entity_summary:\n","    entity_df = pd.DataFrame(entity_summary)\n","    display(HTML(entity_df.to_html(index=False)))"]},{"cell_type":"markdown","metadata":{"id":"MmH1HXPews92"},"source":["## 10. PDF and OCR Processing <a id='pdf-processing'></a>\n","\n","Process scanned documents and PDFs with OCR capabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tV_wDkNOws92"},"outputs":[],"source":["# Check if sample PDF exists\n","pdf_path = Path(\"/content/drive/MyDrive/HoneyBee_MayoWorkshop/HoneyBee-Workshop/examples/samples/sample.PDF\")\n","\n","if pdf_path.exists():\n","    print(\"üìÑ Processing PDF Document\")\n","    print(\"=\" * 60)\n","\n","    # Configure for OCR processing\n","    ocr_config = {\n","        \"document_processor\": {\n","            \"use_ocr\": True,\n","            \"confidence_threshold\": 60,\n","            \"preprocessing\": True\n","        },\n","        \"entity_recognition\": {\n","            \"cancer_specific_extraction\": True\n","        }\n","    }\n","\n","    processor = ClinicalProcessor(config=ocr_config)\n","\n","    try:\n","        # Process PDF\n","        pdf_result = processor.process(pdf_path)\n","\n","        # Display processing information\n","        doc_info = pdf_result.get('document_processing', {})\n","        print(f\"\\nüìä Document Processing Info:\")\n","        print(f\"  - Extraction method: {doc_info.get('method', 'unknown')}\")\n","        print(f\"  - Confidence score: {doc_info.get('confidence', 'N/A')}\")\n","        print(f\"  - Text length: {len(pdf_result.get('text', ''))} characters\")\n","        print(f\"  - Entities found: {len(pdf_result.get('entities', []))}\")\n","\n","        # Show extracted text preview\n","        text = pdf_result.get('text', '')\n","        if text:\n","            print(f\"\\nüìù Extracted Text Preview (first 500 chars):\")\n","            print(\"=\" * 40)\n","            preview = text[:500].replace('\\n', ' ').strip()\n","            print(preview + \"...\" if len(text) > 500 else preview)\n","\n","        # Show any extracted entities\n","        entities = pdf_result.get('entities', [])\n","        if entities:\n","            print(f\"\\nüîç Sample Entities from PDF:\")\n","            for entity in entities[:5]:  # Show first 5\n","                print(f\"  - {entity['type']}: {entity['text']}\")\n","\n","    except Exception as e:\n","        print(f\"\\n‚ùå Error processing PDF: {e}\")\n","        print(\"\\nNote: PDF processing requires:\")\n","        print(\"  - pytesseract (OCR engine)\")\n","        print(\"  - pdf2image (PDF to image conversion)\")\n","        print(\"  - Tesseract OCR installed on system\")\n","else:\n","    print(\"‚ùå Sample PDF not found at expected location.\")\n","    print(f\"   Expected: {pdf_path}\")\n","\n","    # Create a mock scanned document example\n","    print(\"\\nüìÑ Mock OCR Example:\")\n","    print(\"=\" * 60)\n","\n","    # Simulate OCR-like text with some typical OCR issues\n","    ocr_text = \"\"\"\n","    PATIENT NAME: John 5mith\n","    DATE: 03/l5/2023\n","\n","    DIAGN0SIS: Breast ca, Stage IIA\n","\n","    TREATMENT:\n","    - Tamoxifen 20mg_ daily\n","    - F/u in 3 mos\n","    \"\"\"\n","\n","    processor = ClinicalProcessor()\n","    result = processor.process_text(ocr_text, \"scanned_document\")\n","\n","    print(\"\\nProcessing mock OCR text with typical OCR errors...\")\n","    print(f\"Entities found: {len(result.get('entities', []))}\")\n","\n","    for entity in result.get('entities', []):\n","        print(f\"  - {entity['type']}: {entity['text']}\")"]},{"cell_type":"markdown","metadata":{"id":"K2v0Vq1_ws92"},"source":["## 11. Visualization and Analysis <a id='visualization'></a>\n","\n","Comprehensive analysis and visualization of clinical processing results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gR7dJt7kws92"},"outputs":[],"source":["# Process multiple document types for analysis\n","document_types = {\n","    \"Clinical Note\": sample_clinical_text,\n","    \"Pathology Report\": pathology_text,\n","    \"Treatment Timeline\": timeline_text\n","}\n","\n","analysis_results = []\n","all_entities = []\n","\n","processor = ClinicalProcessor({\n","    \"entity_recognition\": {\n","        \"cancer_specific_extraction\": True,\n","        \"temporal_extraction\": True\n","    }\n","})\n","\n","for doc_type, text in document_types.items():\n","    result = processor.process_text(text, doc_type.lower().replace(' ', '_'))\n","    stats = processor.get_summary_statistics(result)\n","\n","    analysis_results.append({\n","        'Document Type': doc_type,\n","        'Text Length': stats['text_length'],\n","        'Total Entities': stats['num_entities'],\n","        'Entity Types': len(stats['entity_types']),\n","        'Timeline Events': stats['num_timeline_events']\n","    })\n","\n","    # Collect all entities for type analysis\n","    for entity in result.get('entities', []):\n","        all_entities.append({\n","            'Document': doc_type,\n","            'Type': entity['type'],\n","            'Text': entity['text']\n","        })\n","\n","# Create summary DataFrame\n","summary_df = pd.DataFrame(analysis_results)\n","entities_df = pd.DataFrame(all_entities)\n","\n","# Display summary\n","print(\"üìä Clinical Processing Summary Analysis\")\n","print(\"=\" * 60)\n","display(HTML(summary_df.to_html(index=False)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GzO6siY_ws93"},"outputs":[],"source":["# Create visualizations\n","fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","\n","# 1. Document comparison\n","ax1 = axes[0, 0]\n","summary_df.plot(x='Document Type', y=['Total Entities', 'Timeline Events'],\n","                kind='bar', ax=ax1)\n","ax1.set_title('Entity and Timeline Extraction by Document Type')\n","ax1.set_xlabel('Document Type')\n","ax1.set_ylabel('Count')\n","ax1.legend(['Total Entities', 'Timeline Events'])\n","ax1.tick_params(axis='x', rotation=45)\n","\n","# 2. Entity type distribution\n","ax2 = axes[0, 1]\n","if not entities_df.empty:\n","    entity_type_counts = entities_df['Type'].value_counts()\n","    ax2.pie(entity_type_counts.values, labels=entity_type_counts.index, autopct='%1.1f%%')\n","    ax2.set_title('Overall Entity Type Distribution')\n","else:\n","    ax2.text(0.5, 0.5, 'No entities found', ha='center', va='center')\n","    ax2.set_title('Entity Type Distribution')\n","\n","# 3. Entity types by document\n","ax3 = axes[1, 0]\n","if not entities_df.empty:\n","    entity_pivot = entities_df.groupby(['Document', 'Type']).size().unstack(fill_value=0)\n","    entity_pivot.plot(kind='bar', stacked=True, ax=ax3)\n","    ax3.set_title('Entity Types by Document')\n","    ax3.set_xlabel('Document Type')\n","    ax3.set_ylabel('Entity Count')\n","    ax3.tick_params(axis='x', rotation=45)\n","    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","else:\n","    ax3.text(0.5, 0.5, 'No entities found', ha='center', va='center')\n","    ax3.set_title('Entity Types by Document')\n","\n","# 4. Processing metrics\n","ax4 = axes[1, 1]\n","metrics = summary_df[['Text Length', 'Total Entities']].values\n","if metrics.size > 0:\n","    ax4.scatter(metrics[:, 0], metrics[:, 1], s=100)\n","    for i, doc in enumerate(summary_df['Document Type']):\n","        ax4.annotate(doc, (metrics[i, 0], metrics[i, 1]),\n","                    xytext=(5, 5), textcoords='offset points')\n","    ax4.set_xlabel('Text Length (characters)')\n","    ax4.set_ylabel('Total Entities')\n","    ax4.set_title('Entity Extraction Efficiency')\n","    ax4.grid(True, alpha=0.3)\n","else:\n","    ax4.text(0.5, 0.5, 'No data available', ha='center', va='center')\n","    ax4.set_title('Entity Extraction Efficiency')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AS6p5YT4ws93"},"outputs":[],"source":["# Generate final summary report\n","print(\"\\nüìã CLINICAL PROCESSING CAPABILITIES SUMMARY\")\n","print(\"=\" * 60)\n","\n","capabilities = [\n","    (\"‚úÖ\", \"Multiple input formats (PDF, images, EHR formats)\"),\n","    (\"‚úÖ\", \"OCR with medical terminology verification\"),\n","    (\"‚úÖ\", \"Biomedical language models (GatorTron, Bio-ClinicalBERT, etc.)\"),\n","    (\"‚úÖ\", \"Cancer-specific entity extraction\"),\n","    (\"‚úÖ\", \"Temporal timeline construction\"),\n","    (\"‚úÖ\", \"Medical abbreviation expansion\"),\n","    (\"‚úÖ\", \"Ontology linking (SNOMED-CT, RxNorm, LOINC)\"),\n","    (\"‚úÖ\", \"Long document handling strategies\"),\n","    (\"‚úÖ\", \"Batch processing capabilities\"),\n","    (\"‚úÖ\", \"Configurable processing pipelines\")\n","]\n","\n","for icon, capability in capabilities:\n","    print(f\"{icon} {capability}\")\n","\n","print(\"\\nüìä Processing Statistics from Examples:\")\n","if not entities_df.empty:\n","    print(f\"  - Total documents processed: {len(document_types)}\")\n","    print(f\"  - Total entities extracted: {len(entities_df)}\")\n","    print(f\"  - Unique entity types: {entities_df['Type'].nunique()}\")\n","    print(f\"  - Most common entity type: {entities_df['Type'].mode()[0]}\")\n","\n","print(\"\\nüéØ Key Use Cases:\")\n","use_cases = [\n","    \"Clinical trial eligibility screening\",\n","    \"Treatment response monitoring\",\n","    \"Quality measure reporting\",\n","    \"Research cohort identification\",\n","    \"Automated clinical documentation\"\n","]\n","\n","for use_case in use_cases:\n","    print(f\"  ‚Ä¢ {use_case}\")\n","\n","print(\"\\n‚ú® This notebook demonstrated the complete clinical processing pipeline in HoneyBee!\")"]}],"metadata":{"kernelspec":{"display_name":"HoneyBee","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}